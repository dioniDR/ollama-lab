# Ollama Lab ğŸ¤–

Una interfaz web moderna y completa para interactuar con modelos de Ollama, con gestiÃ³n avanzada de prompts y controles de chat mejorados.

## ğŸŒŸ CaracterÃ­sticas

### ğŸ’¬ Chat Interactivo
- **Interfaz web moderna** con diseÃ±o oscuro
- **Streaming en tiempo real** de respuestas
- **Detener respuesta** en cualquier momento
- **Limpiar chat** para reiniciar el contexto
- **MÃºltiples modelos** disponibles

### ğŸ¯ GestiÃ³n de Prompts
- **Guardar prompts** del sistema con nombres y descripciones
- **IDs Ãºnicos** para cada prompt
- **NavegaciÃ³n fÃ¡cil** entre prompts guardados
- **Aplicar prompts** con un solo clic
- **Eliminar prompts** no deseados
- **Metadatos** con fechas de creaciÃ³n y Ãºltimo uso

### âš™ï¸ ConfiguraciÃ³n Avanzada
- **Temperatura** (0.0 - 2.0)
- **Top P** (0.0 - 1.0)
- **Contexto** personalizable
- **PenalizaciÃ³n de repeticiÃ³n**
- **Cambio de modelo** en tiempo real

### ğŸ›ï¸ Controles de Chat
- **BotÃ³n de parar** para detener respuestas
- **BotÃ³n de limpiar** para reiniciar conversaciones
- **PrevenciÃ³n de mÃºltiples requests** simultÃ¡neos
- **Indicadores visuales** del estado actual

## ğŸš€ InstalaciÃ³n

### Prerrequisitos
- Python 3.8+
- [Ollama](https://ollama.ai/) instalado y ejecutÃ¡ndose

### ConfiguraciÃ³n

1. **Clona el repositorio:**
```bash
git clone https://github.com/dioniDR/ollama-lab.git
cd ollama-lab
```

2. **Crea un entorno virtual:**
```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

3. **Instala las dependencias:**
```bash
pip install -r requirements.txt
```

4. **AsegÃºrate de que Ollama estÃ© ejecutÃ¡ndose:**
```bash
# En otra terminal
ollama serve
```

5. **Ejecuta la aplicaciÃ³n:**
```bash
python run.py
```

6. **Abre tu navegador** en `http://localhost:8000`

## ğŸ“‹ Uso

### ConfiguraciÃ³n Inicial
1. Haz clic en **"ConfiguraciÃ³n"** para ajustar parÃ¡metros del modelo
2. Selecciona tu modelo preferido desde el selector
3. Configura el prompt del sistema segÃºn tus necesidades

### GestiÃ³n de Prompts
1. Haz clic en **"Prompts"** para abrir el gestor
2. **Crear nuevo prompt:**
   - Ingresa nombre y descripciÃ³n
   - Escribe el prompt del sistema
   - Haz clic en "Guardar Prompt"
3. **Usar prompt guardado:**
   - Selecciona de la lista de prompts guardados
   - Haz clic en "Usar"
4. **Eliminar prompt:**
   - Haz clic en "Eliminar" junto al prompt deseado

### Controles de Chat
- **Enviar mensaje:** Escribe y presiona Enter o haz clic en "Enviar"
- **Detener respuesta:** Haz clic en "Detener" (botÃ³n rojo) mientras se genera
- **Limpiar chat:** Haz clic en "Limpiar Chat" para reiniciar la conversaciÃ³n

## ğŸ”§ API Endpoints

### Chat
- `POST /chat` - Enviar mensaje al modelo
- `GET /models` - Listar modelos disponibles
- `POST /switch-model` - Cambiar modelo activo

### ConfiguraciÃ³n
- `GET /config` - Obtener configuraciÃ³n actual
- `POST /config` - Actualizar configuraciÃ³n

### Prompts
- `GET /prompts` - Listar todos los prompts guardados
- `POST /prompts` - Guardar nuevo prompt
- `POST /prompts/use` - Aplicar prompt especÃ­fico
- `GET /prompts/{id}` - Obtener prompt especÃ­fico
- `DELETE /prompts/{id}` - Eliminar prompt

## ğŸ“ Estructura del Proyecto

```
ollama-lab/
â”œâ”€â”€ main.py              # AplicaciÃ³n FastAPI principal
â”œâ”€â”€ run.py               # Script de ejecuciÃ³n
â”œâ”€â”€ requirements.txt     # Dependencias Python
â”œâ”€â”€ config.json          # ConfiguraciÃ³n del modelo
â”œâ”€â”€ saved_prompts.json   # Prompts guardados
â”œâ”€â”€ static/
â”‚   â””â”€â”€ index.html       # Interfaz web
â””â”€â”€ venv/               # Entorno virtual
```

## ğŸ› ï¸ TecnologÃ­as

- **Backend:** FastAPI, Python 3.8+
- **Frontend:** HTML5, CSS3, JavaScript (Vanilla)
- **API:** Ollama REST API
- **Streaming:** Server-Sent Events
- **Persistencia:** JSON files

## ğŸ¯ CaracterÃ­sticas TÃ©cnicas

### Backend
- **Streaming asÃ­ncrono** con FastAPI
- **Proxy transparente** hacia Ollama
- **GestiÃ³n de configuraciÃ³n** persistente
- **Sistema de prompts** con UUIDs Ãºnicos
- **Manejo robusto de errores**

### Frontend
- **Interfaz responsiva** con CSS Grid/Flexbox
- **Streaming en tiempo real** con Fetch API
- **GestiÃ³n de estado** con JavaScript
- **Controles de cancelaciÃ³n** con AbortController
- **Modales y formularios** dinÃ¡micos

## ğŸ”„ Flujo de Trabajo

1. **ConfiguraciÃ³n inicial** del modelo y prompts
2. **Inicio de conversaciÃ³n** con prompt del sistema
3. **Intercambio de mensajes** con streaming
4. **Control de respuestas** (detener/continuar)
5. **GestiÃ³n de contexto** (limpiar/reiniciar)

## ğŸ“ ConfiguraciÃ³n por Defecto

```json
{
  "model": "llama3.2",
  "temperature": 0.7,
  "top_p": 0.9,
  "system_prompt": "You are a helpful assistant.",
  "max_tokens": 4096,
  "stream": true,
  "num_ctx": 2048,
  "repeat_penalty": 1.1
}
```

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -m 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

## ğŸ› Reporte de Bugs

Si encuentras algÃºn bug, por favor abre un [issue](https://github.com/dioniDR/ollama-lab/issues) con:
- DescripciÃ³n del problema
- Pasos para reproducir
- Comportamiento esperado
- Screenshots (si aplica)

## ğŸš§ Roadmap

- [ ] Historial de conversaciones
- [ ] Exportar conversaciones
- [ ] Temas personalizables
- [ ] Soporte para imÃ¡genes
- [ ] IntegraciÃ³n con mÃ¡s modelos
- [ ] API REST completa

## ğŸ‘¤ Autor

**Dioni DR**
- GitHub: [@dioniDR](https://github.com/dioniDR)

---

â­ **Â¡Si te gusta el proyecto, no olvides darle una estrella!**